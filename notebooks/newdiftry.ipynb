{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Diffusion try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "import zipfile\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch import einsum\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import torchvision.transforms as T\n",
    "from contextlib import nullcontext\n",
    "from fastprogress import progress_bar\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, num_samples, n_targets, random_n_targets, nu=None, scnr=None, snr=None, cnr=None):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.n_targets = n_targets\n",
    "        self.random_n_targets = random_n_targets\n",
    "        self.with_targets = n_targets > 0\n",
    "        self.snr_dB = snr\n",
    "        self.cnr_dB = cnr\n",
    "        self.scnr = scnr\n",
    "        self.nu = torch.tensor([nu], dtype=torch.float) if nu is not None else None\n",
    "\n",
    "        # Radar parameters\n",
    "        self.N = 64       # fast–time samples per pulse\n",
    "        self.K = 64       # slow–time pulses per frame\n",
    "        self.B = 50e6     # Chirp bandwidth (Hz)\n",
    "        self.T0 = 1e-3    # PRI (s)\n",
    "        self.fc = 9.39e9  # Carrier frequency (Hz)\n",
    "        self.c = 3e8      # Speed of light (m/s)\n",
    "        self.CNR = 15     # in dB (only used if snr/cnr are NOT given)\n",
    "\n",
    "        # Range and Doppler settings\n",
    "        self.r_min, self.r_max = 0, 189    # meters\n",
    "        self.v_min, self.v_max = -7.8, 7.8   # m/s (for targets)\n",
    "        self.vc_min, self.vc_max = -7.8, 7.8 # m/s (for clutter)\n",
    "        self.dr = 3       # Range resolution in m\n",
    "        self.dv = 0.249   # Doppler resolution in m/s\n",
    "\n",
    "        # Range and Doppler bins (for label maps)\n",
    "        self.R = torch.arange(self.r_min, self.r_max + self.dr, self.dr)\n",
    "        self.V = torch.arange(self.v_min, self.v_max + self.dv, self.dv)\n",
    "        self.dR = len(self.R)\n",
    "        self.dV = len(self.V)\n",
    "\n",
    "        # Noise power calculation (only used if snr/cnr not specified)\n",
    "        self.sigma2 = self.N / (2 * 10 ** (self.CNR / 10))\n",
    "        \n",
    "        # For old scnr logic, we also computed a \"normalization\" factor\n",
    "        self.cn_norm = torch.sqrt(\n",
    "            torch.tensor(\n",
    "                self.N * self.K * (self.N // 2 + self.sigma2), dtype=torch.float\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def generate_target_signal(self, ranges, velocities, phases, gains_dB):\n",
    "        \"\"\"\n",
    "        Creates the raw (unscaled) target signals for each target,\n",
    "        then (if old scnr approach) scales them to achieve the desired scnr in dB,\n",
    "        or returns them raw for later scaling if snr/cnr approach is used.\n",
    "        \"\"\"\n",
    "        # Range steering vector (one per target)\n",
    "        w_r = (2 * torch.pi * 2 * self.B * ranges) / (self.c * self.N)\n",
    "        range_steering = torch.exp(-1j * torch.outer(w_r, torch.arange(self.N, dtype=torch.float)))\n",
    "        \n",
    "        # Doppler steering vector (one per target)\n",
    "        w_d = (2 * torch.pi * self.T0 * 2 * self.fc * velocities) / self.c\n",
    "        doppler_steering = torch.exp(-1j * torch.outer(w_d, torch.arange(self.K, dtype=torch.float)))\n",
    "        \n",
    "        # Form the fast–time × slow–time target signature for each target\n",
    "        rd_signal = range_steering.unsqueeze(-1) * doppler_steering.unsqueeze(1)\n",
    "        rd_signal = rd_signal * torch.exp(1j * phases)  # impart random phase per target\n",
    "        \n",
    "        # If we are using the old SCNR approach, scale immediately\n",
    "        # Gains in dB => each target’s SCNR\n",
    "        if (self.snr_dB is None) or (self.cnr_dB is None):\n",
    "            # Old approach: sum all scaled targets into a single matrix\n",
    "            S_norm = torch.linalg.norm(rd_signal, dim=(1, 2)).real\n",
    "            sig_amp = (10 ** (gains_dB / 20)) * (self.cn_norm / S_norm)\n",
    "            rd_signal = (sig_amp.unsqueeze(-1).unsqueeze(-1) * rd_signal).sum(dim=0)\n",
    "            return rd_signal\n",
    "        else:\n",
    "            # With the new approach, we do NOT scale by SCNR here.\n",
    "            # We return the raw sum across all targets, for later power-based scaling.\n",
    "            rd_signal = rd_signal.sum(dim=0)\n",
    "            return rd_signal\n",
    "\n",
    "    def generate_clutter(self, nu):\n",
    "        # Choose a clutter velocity uniformly within the allowed limits\n",
    "        clutter_vel = torch.empty(1).uniform_(self.vc_min, self.vc_max)\n",
    "        fd = 2 * torch.pi * (2 * self.fc * clutter_vel) / self.c \n",
    "        sigma_f = 0.05  # Correlation parameter (from the referenced paper)\n",
    "\n",
    "        p, q = torch.meshgrid(\n",
    "            torch.arange(self.N, dtype=torch.float),\n",
    "            torch.arange(self.K, dtype=torch.float),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        # M is the covariance-like matrix for correlated clutter\n",
    "        M = torch.exp(\n",
    "            -2 * torch.pi**2 * sigma_f**2 * (p - q)**2\n",
    "            - 1j * (p - q) * fd * self.T0\n",
    "        )\n",
    "\n",
    "        # Draw complex Gaussian\n",
    "        z = torch.randn(self.K, self.dR, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2.0))\n",
    "        e, V_mat = torch.linalg.eigh(M)  # eigen-decomposition\n",
    "        e_sqrt = torch.sqrt(torch.clamp(e.real, min=0.0))\n",
    "        E = torch.diag(e_sqrt)\n",
    "        A = V_mat @ E.to(V_mat.dtype)\n",
    "        w_t = A @ z  # shaping the random draws to match M\n",
    "\n",
    "        # Impart heavy–tailed behavior via Gamma modulation (shape = scale = nu)\n",
    "        s = torch.distributions.Gamma(nu, nu).sample((self.dR,))\n",
    "        c_t = (torch.sqrt(s).unsqueeze(0) * w_t.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Convert to fast–time × slow–time representation\n",
    "        # using a range–steering operation.\n",
    "        c_r_steer = torch.exp(\n",
    "            -1j \n",
    "            * 2 \n",
    "            * torch.pi \n",
    "            * torch.outer(torch.arange(self.N, dtype=torch.float), self.R)\n",
    "            * (2 * self.B) / (self.c * self.N)\n",
    "        )\n",
    "        C = c_r_steer @ c_t.transpose(0, 1)\n",
    "        return C\n",
    "\n",
    "    def gen_frame_and_labels(self):\n",
    "        \"\"\"\n",
    "        Generate one radar data frame, label map, and the separate S, C, W\n",
    "        so that we can control SNR and CNR (if specified).\n",
    "        \"\"\"\n",
    "        # 1. Generate unscaled noise (mean 0, unit variance in each real/imag component).\n",
    "        #    We'll measure it and scale later if snr/cnr is used.\n",
    "        W_unscaled = torch.randn(self.N, self.K, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2.0))\n",
    "        \n",
    "        # 2. Generate unscaled clutter\n",
    "        nu = torch.empty(1).uniform_(0.1, 1.5) if self.nu is None else self.nu\n",
    "        C_unscaled = self.generate_clutter(nu)\n",
    "        \n",
    "        # 3. Prepare to generate target signal(s)\n",
    "        #    We'll choose random targets if with_targets == True\n",
    "        S_unscaled = torch.zeros(self.N, self.K, dtype=torch.cfloat)\n",
    "        rd_label = torch.zeros(self.dR, self.dV)\n",
    "\n",
    "        if self.with_targets:\n",
    "            n = (\n",
    "                torch.randint(1, self.n_targets + 1, (1,)).item()\n",
    "                if self.random_n_targets\n",
    "                else self.n_targets\n",
    "            )\n",
    "            ranges = torch.empty(n).uniform_(self.r_min, self.r_max)\n",
    "            velocities = torch.empty(n).uniform_(self.v_min, self.v_max)\n",
    "            phases = torch.empty(n, 1, 1).uniform_(0, 2 * torch.pi)\n",
    "            \n",
    "            # If new SNR/CNR approach is NOT used, we fallback to scnr or [-5, 10] dB random\n",
    "            if (self.snr_dB is None) or (self.cnr_dB is None):\n",
    "                SCNR_dBs = torch.empty(n).uniform_(-5, 10) if self.scnr is None else self.scnr * torch.ones(n)\n",
    "                S_unscaled = self.generate_target_signal(ranges, velocities, phases, SCNR_dBs)\n",
    "            else:\n",
    "                # Just pass dummy dB array here; we won't scale inside 'generate_target_signal'\n",
    "                # Instead, we will do the scaling outside\n",
    "                S_raw = []\n",
    "                for i in range(n):\n",
    "                    # Each target can have the same 'gain' placeholder\n",
    "                    s_i = self.generate_target_signal(\n",
    "                        ranges[i].unsqueeze(-1),\n",
    "                        velocities[i].unsqueeze(-1),\n",
    "                        phases[i].unsqueeze(-1),\n",
    "                        gains_dB=torch.tensor([0.0])  # placeholder\n",
    "                    )\n",
    "                    S_raw.append(s_i)\n",
    "                # Sum all targets\n",
    "                S_unscaled = sum(S_raw)\n",
    "\n",
    "            # For each target, mark the closest range and Doppler bin.\n",
    "            for r, v in zip(ranges, velocities):\n",
    "                r_bin = torch.argmin(torch.abs(self.R - r))\n",
    "                v_bin = torch.argmin(torch.abs(self.V - v))\n",
    "                rd_label[r_bin, v_bin] = 1\n",
    "\n",
    "        # ---------------------------\n",
    "        # NEW: If snr & cnr are given, do amplitude scaling here\n",
    "        # ---------------------------\n",
    "        if (self.snr_dB is not None) and (self.cnr_dB is not None):\n",
    "            # 1) measure raw powers\n",
    "            noise_power  = W_unscaled.abs().pow(2).mean()\n",
    "            clutter_power= C_unscaled.abs().pow(2).mean() if C_unscaled.numel() > 0 else 0.0\n",
    "            signal_power = S_unscaled.abs().pow(2).mean() if S_unscaled.numel() > 0 else 0.0\n",
    "\n",
    "            # 2) define desired linear ratios\n",
    "            snr_lin = 10 ** (self.snr_dB / 10)\n",
    "            cnr_lin = 10 ** (self.cnr_dB / 10)\n",
    "\n",
    "            # 3) define desired final powers\n",
    "            #    We'll anchor the noise to \"1.0\" average power for convenience\n",
    "            #    (or you could anchor it to some other power). Then scale clutter & signal.\n",
    "            #    Step (A): Scale noise to final_noise_power = 1.0\n",
    "            #             => alpha_n = sqrt(1 / noise_power).\n",
    "            alpha_n = torch.sqrt(1.0 / noise_power)\n",
    "            W = alpha_n * W_unscaled  # final noise\n",
    "            final_noise_power = W.abs().pow(2).mean()\n",
    "\n",
    "            #    Step (B): Clutter should have average power = cnr_lin * final_noise_power\n",
    "            if clutter_power > 0:\n",
    "                alpha_c = torch.sqrt((cnr_lin * final_noise_power) / clutter_power)\n",
    "                C = alpha_c * C_unscaled\n",
    "            else:\n",
    "                C = torch.zeros_like(C_unscaled)\n",
    "\n",
    "            #    Step (C): Signal should have average power = snr_lin * final_noise_power\n",
    "            if signal_power > 0:\n",
    "                alpha_s = torch.sqrt((snr_lin * final_noise_power) / signal_power)\n",
    "                S = alpha_s * S_unscaled\n",
    "            else:\n",
    "                S = torch.zeros_like(S_unscaled)\n",
    "\n",
    "        else:\n",
    "            W = (W_unscaled / torch.sqrt(torch.tensor(self.sigma2)))  # old approach\n",
    "            C = C_unscaled\n",
    "            S = S_unscaled\n",
    "\n",
    "        X = S + C + W\n",
    "        \n",
    "        signal_energy  = S.abs().pow(2).sum()\n",
    "        clutter_energy = C.abs().pow(2).sum()\n",
    "        noise_energy   = W.abs().pow(2).sum()\n",
    "        scnr_lin = signal_energy / (clutter_energy + noise_energy + 1e-12)\n",
    "        scnr_dB  = 10.0 * torch.log10(scnr_lin + 1e-12)\n",
    "\n",
    "        return S, C, W, X, rd_label, scnr_dB\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Return the data and label for sample 'idx'.\n",
    "        \"\"\"\n",
    "        signal, clutter, gaus_noise, IQ, rd_label, scnr_dB = self.gen_frame_and_labels()\n",
    "        return signal, clutter, gaus_noise, IQ, rd_label, scnr_dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s, reproducible=False):\n",
    "    \"set random seed for torch, numpy and random\"\n",
    "    try: torch.manual_seed(s)\n",
    "    except NameError: pass\n",
    "    try: torch.cuda.manual_seed_all(s)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(s%(2**32-1)) #numpy demands seed between 0 and 2**32-1\n",
    "    except NameError: pass\n",
    "    random.seed(s)\n",
    "    if reproducible:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(args):\n",
    "    dataset_with_targets = RadarDataset(num_samples=args.num_samples, n_targets=args.n_targets, random_n_targets=True, snr=args.snr, cnr=args.cnr)\n",
    "    dataset_without = RadarDataset(num_samples=args.num_samples, n_targets=args.n_targets, random_n_targets=True, snr=args.snr, cnr=args.cnr)\n",
    "    full_dataset = ConcatDataset([dataset_with_targets, dataset_without])\n",
    "    train_size = int(0.9*len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folders(run_name):\n",
    "    \"create folders for models and results, and subfolders for each run\"\n",
    "    os.makedirs(\"Models\", exist_ok=True)\n",
    "    os.makedirs(\"Results\", exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"Models\", run_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"Results\", run_name), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_param(m):\n",
    "    \"get model first parameter\"\n",
    "    return next(iter(m.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"Exponential Moving Average, used to make training more stable\"\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    \n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "    \n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "    \n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True) # 4 attention heads, input tensors shape (batch, seq_len, emb_dim)\n",
    "        self.ln = nn.layerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-1]\n",
    "        x = x.view(-1, self.channels, size * size).swapaxes(1, 2) # (batch, channels, H, W) -> (batch, channels, H*W) -> (batch, H*W, channels) which is required for MHD\n",
    "        x_ln = self.ln(x) # normalized input\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln) # compute self-attention where x_ln is query, key and value\n",
    "        attention_value = attention_value + x # residual connection\n",
    "        attention_value = self.ff_self(attention_value) + attention_value # feed forward network and residual connection again\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, size, size) # reshape back to (batch, channels, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(emb_dim, out_channels)\n",
    "        )\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:,:, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x+emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels//2)\n",
    "        )\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(emb_dim, out_channels)\n",
    "        )\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:,:,None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x+emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=2, c_out=2, time_dim=256, remove_deep_conv=False):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.remove_deep_conv = remove_deep_conv\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256)\n",
    "        if remove_deep_conv:\n",
    "            self.bot1 = DoubleConv(256, 256)\n",
    "            self.bot3 = DoubleConv(256, 256)\n",
    "        else:\n",
    "            self.bot1 = DoubleConv(256, 512)\n",
    "            self.bot2 = DoubleConv(512, 512)\n",
    "            self.bot3 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64)\n",
    "        self.up3 = Up(128,64)\n",
    "        self.sa6 = SelfAttention(64)\n",
    "        self.outc = DoubleConv(64, c_out, Kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=one_param(self).device).float()/channels))\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels //2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels //2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def unet_forward(self, x, t):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        if not self.remove_deep_conv:\n",
    "            x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        return self.unet_forward(x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_UNet(UNet):\n",
    "    def __init__(self, c_in=2, c_out=2, time_dim=256, cond_in=2, **kwargs):\n",
    "        super().__init__(c_in + cond_in, c_out, time_dim, **kwargs)\n",
    "    \n",
    "    def forward(self, x, cond_img, t):\n",
    "        t = t.unsqueeze(-1) if t.dim() == 1 else t\n",
    "        t = self.pos_encoding(t, self.time_dim)  # e.g., sinusoidal or learned pos enc\n",
    "        x_input = torch.cat([x, cond_img], dim=1)\n",
    "\n",
    "        return self.unet_forward(x_input, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    run_name = \"Conditional_Radar_Diffusion_v1\",\n",
    "    epochs = 100,\n",
    "    noise_steps =1000,\n",
    "    seed = 42,\n",
    "    batch_size = 10,\n",
    "    img_size = 64,\n",
    "    device = \"cuda\",\n",
    "    slice_size = 1,\n",
    "    do_validation = True,\n",
    "    fp16 = True,\n",
    "    log_every_epoch = 10,\n",
    "    num_workers=10,\n",
    "    lr = 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, c_in=2, c_out=2, device=\"cuda\", **kwargs):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "        self.model = Conditional_UNet(c_in, c_out)\n",
    "        self.ema_model = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "        self.device = device\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "    \n",
    "    def noise_images(self, x, t):\n",
    "        \"add noise to images at instant t\"\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:,None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1-self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.rand_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "    \n",
    "    def train_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.ema.step_ema(self.ema_model, self.model)\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def load(self, model_ckpt_path, model_ckpt=\"ckpt.pt\", ema_model_ckpt=\"ema_ckpt.pt\"):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(model_ckpt_path, model_ckpt)))\n",
    "        self.ema_model.load_state_dict(torch.load(os.path.join(model_ckpt_path, ema_model_ckpt)))\n",
    "\n",
    "    def save_model(self, run_name, epoch=-1):\n",
    "        \"save model locally\"\n",
    "        torch.save(self.model.state_dict(), os.path.join(\"models\", run_name, f\"ckpt.pt\"))\n",
    "        torch.save(self.ema_model.state_dict(), os.path.join(\"models\", run_name, f\"ema_ckpt.pt\"))\n",
    "        torch.save(self.optimizer.state_dict(), os.path.join(\"models\", run_name, f\"optim.pt\"))\n",
    "\n",
    "    def prepare(self, args):\n",
    "        make_folders(args.run_name)\n",
    "        self.train_dataloader, self.val_dataloader = get_data(args)\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=args.lr, eps=1e-5)\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.lr, steps_per_epoch=len(self.train_dataloader), epochs=args.epochs)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ema = EMA(0.995)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def fit(self, args):\n",
    "        for epoch in progress_bar(range(args.epochs), total=args.epochs, leave=True):\n",
    "            _ = self.one_epoch(train=True)\n",
    "\n",
    "            if args.do_validation:\n",
    "                avg_loss = self.one_epoch(train=False)\n",
    "            \n",
    "        self.save_model(run_name=args.run_name, epoch=epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def one_epoch(self, train=True):\n",
    "        avg_loss = 0.\n",
    "        if train: self.model.train()\n",
    "        else: self.model.eval()\n",
    "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
    "        for i, (images, nimages) in enumerate(pbar):\n",
    "            with torch.autocast(\"cuda\") and (torch.inference_mode() if not train else torch.enable_grad()):\n",
    "                images = images.to(self.device)\n",
    "                nimages = nimages.to(self.device)\n",
    "                t = self.sample_timesteps(images.shape[0]).to(self.device)\n",
    "                x_t, noise = self.noise_images(images, t)\n",
    "                predicted_noise = self.model(x_t, t, nimages)\n",
    "                loss = self.mse(noise, predicted_noise)\n",
    "                avg_loss += loss\n",
    "            if train:\n",
    "                self.train_step(loss)\n",
    "            pbar.comment = f\"MSE={loss.item():2.3f}\"\n",
    "        return avg_loss.mean().item()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
