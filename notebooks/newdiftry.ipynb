{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Diffusion try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "import zipfile\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch import einsum\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "import torchvision.transforms as T\n",
    "from contextlib import nullcontext\n",
    "from fastprogress import progress_bar\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, num_samples, n_targets, random_n_targets, nu=None, scnr=None, snr=None, cnr=None):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.n_targets = n_targets\n",
    "        self.random_n_targets = random_n_targets\n",
    "        self.with_targets = n_targets > 0\n",
    "        self.snr_dB = snr\n",
    "        self.cnr_dB = cnr\n",
    "        self.scnr = scnr\n",
    "        self.nu = torch.tensor([nu], dtype=torch.float) if nu is not None else None\n",
    "\n",
    "        # Radar parameters\n",
    "        self.N = 64       # fast–time samples per pulse\n",
    "        self.K = 64       # slow–time pulses per frame\n",
    "        self.B = 50e6     # Chirp bandwidth (Hz)\n",
    "        self.T0 = 1e-3    # PRI (s)\n",
    "        self.fc = 9.39e9  # Carrier frequency (Hz)\n",
    "        self.c = 3e8      # Speed of light (m/s)\n",
    "        self.CNR = 15     # in dB (only used if snr/cnr are NOT given)\n",
    "\n",
    "        # Range and Doppler settings\n",
    "        self.r_min, self.r_max = 0, 189    # meters\n",
    "        self.v_min, self.v_max = -7.8, 7.8   # m/s (for targets)\n",
    "        self.vc_min, self.vc_max = -7.8, 7.8 # m/s (for clutter)\n",
    "        self.dr = 3       # Range resolution in m\n",
    "        self.dv = 0.249   # Doppler resolution in m/s\n",
    "\n",
    "        # Range and Doppler bins (for label maps)\n",
    "        self.R = torch.arange(self.r_min, self.r_max + self.dr, self.dr)\n",
    "        self.V = torch.arange(self.v_min, self.v_max + self.dv, self.dv)\n",
    "        self.dR = len(self.R)\n",
    "        self.dV = len(self.V)\n",
    "\n",
    "        # Noise power calculation (only used if snr/cnr not specified)\n",
    "        self.sigma2 = self.N / (2 * 10 ** (self.CNR / 10))\n",
    "        \n",
    "        # For old scnr logic, we also computed a \"normalization\" factor\n",
    "        self.cn_norm = torch.sqrt(\n",
    "            torch.tensor(\n",
    "                self.N * self.K * (self.N // 2 + self.sigma2), dtype=torch.float\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def generate_target_signal(self, ranges, velocities, phases, gains_dB):\n",
    "        \"\"\"\n",
    "        Creates the raw (unscaled) target signals for each target,\n",
    "        then (if old scnr approach) scales them to achieve the desired scnr in dB,\n",
    "        or returns them raw for later scaling if snr/cnr approach is used.\n",
    "        \"\"\"\n",
    "        # Range steering vector (one per target)\n",
    "        w_r = (2 * torch.pi * 2 * self.B * ranges) / (self.c * self.N)\n",
    "        range_steering = torch.exp(-1j * torch.outer(w_r, torch.arange(self.N, dtype=torch.float)))\n",
    "        \n",
    "        # Doppler steering vector (one per target)\n",
    "        w_d = (2 * torch.pi * self.T0 * 2 * self.fc * velocities) / self.c\n",
    "        doppler_steering = torch.exp(-1j * torch.outer(w_d, torch.arange(self.K, dtype=torch.float)))\n",
    "        \n",
    "        # Form the fast–time × slow–time target signature for each target\n",
    "        rd_signal = range_steering.unsqueeze(-1) * doppler_steering.unsqueeze(1)\n",
    "        rd_signal = rd_signal * torch.exp(1j * phases)  # impart random phase per target\n",
    "        \n",
    "        # If we are using the old SCNR approach, scale immediately\n",
    "        # Gains in dB => each target’s SCNR\n",
    "        if (self.snr_dB is None) or (self.cnr_dB is None):\n",
    "            # Old approach: sum all scaled targets into a single matrix\n",
    "            S_norm = torch.linalg.norm(rd_signal, dim=(1, 2)).real\n",
    "            sig_amp = (10 ** (gains_dB / 20)) * (self.cn_norm / S_norm)\n",
    "            rd_signal = (sig_amp.unsqueeze(-1).unsqueeze(-1) * rd_signal).sum(dim=0)\n",
    "            return rd_signal\n",
    "        else:\n",
    "            # With the new approach, we do NOT scale by SCNR here.\n",
    "            # We return the raw sum across all targets, for later power-based scaling.\n",
    "            rd_signal = rd_signal.sum(dim=0)\n",
    "            return rd_signal\n",
    "\n",
    "    def generate_clutter(self, nu):\n",
    "        # Choose a clutter velocity uniformly within the allowed limits\n",
    "        clutter_vel = torch.empty(1).uniform_(self.vc_min, self.vc_max)\n",
    "        fd = 2 * torch.pi * (2 * self.fc * clutter_vel) / self.c \n",
    "        sigma_f = 0.05  # Correlation parameter (from the referenced paper)\n",
    "\n",
    "        p, q = torch.meshgrid(\n",
    "            torch.arange(self.N, dtype=torch.float),\n",
    "            torch.arange(self.K, dtype=torch.float),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        # M is the covariance-like matrix for correlated clutter\n",
    "        M = torch.exp(\n",
    "            -2 * torch.pi**2 * sigma_f**2 * (p - q)**2\n",
    "            - 1j * (p - q) * fd * self.T0\n",
    "        )\n",
    "\n",
    "        # Draw complex Gaussian\n",
    "        z = torch.randn(self.K, self.dR, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2.0))\n",
    "        e, V_mat = torch.linalg.eigh(M)  # eigen-decomposition\n",
    "        e_sqrt = torch.sqrt(torch.clamp(e.real, min=0.0))\n",
    "        E = torch.diag(e_sqrt)\n",
    "        A = V_mat @ E.to(V_mat.dtype)\n",
    "        w_t = A @ z  # shaping the random draws to match M\n",
    "\n",
    "        # Impart heavy–tailed behavior via Gamma modulation (shape = scale = nu)\n",
    "        s = torch.distributions.Gamma(nu, nu).sample((self.dR,))\n",
    "        c_t = (torch.sqrt(s).unsqueeze(0) * w_t.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Convert to fast–time × slow–time representation\n",
    "        # using a range–steering operation.\n",
    "        c_r_steer = torch.exp(\n",
    "            -1j \n",
    "            * 2 \n",
    "            * torch.pi \n",
    "            * torch.outer(torch.arange(self.N, dtype=torch.float), self.R)\n",
    "            * (2 * self.B) / (self.c * self.N)\n",
    "        )\n",
    "        C = c_r_steer @ c_t.transpose(0, 1)\n",
    "        return C\n",
    "\n",
    "    def gen_frame_and_labels(self):\n",
    "        \"\"\"\n",
    "        Generate one radar data frame, label map, and the separate S, C, W\n",
    "        so that we can control SNR and CNR (if specified).\n",
    "        \"\"\"\n",
    "        # 1. Generate unscaled noise (mean 0, unit variance in each real/imag component).\n",
    "        #    We'll measure it and scale later if snr/cnr is used.\n",
    "        W_unscaled = torch.randn(self.N, self.K, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2.0))\n",
    "        \n",
    "        # 2. Generate unscaled clutter\n",
    "        nu = torch.empty(1).uniform_(0.1, 1.5) if self.nu is None else self.nu\n",
    "        C_unscaled = self.generate_clutter(nu)\n",
    "        \n",
    "        # 3. Prepare to generate target signal(s)\n",
    "        #    We'll choose random targets if with_targets == True\n",
    "        S_unscaled = torch.zeros(self.N, self.K, dtype=torch.cfloat)\n",
    "        rd_label = torch.zeros(self.dR, self.dV)\n",
    "\n",
    "        if self.with_targets:\n",
    "            n = (\n",
    "                torch.randint(1, self.n_targets + 1, (1,)).item()\n",
    "                if self.random_n_targets\n",
    "                else self.n_targets\n",
    "            )\n",
    "            ranges = torch.empty(n).uniform_(self.r_min, self.r_max)\n",
    "            velocities = torch.empty(n).uniform_(self.v_min, self.v_max)\n",
    "            phases = torch.empty(n, 1, 1).uniform_(0, 2 * torch.pi)\n",
    "            \n",
    "            # If new SNR/CNR approach is NOT used, we fallback to scnr or [-5, 10] dB random\n",
    "            if (self.snr_dB is None) or (self.cnr_dB is None):\n",
    "                SCNR_dBs = torch.empty(n).uniform_(-5, 10) if self.scnr is None else self.scnr * torch.ones(n)\n",
    "                S_unscaled = self.generate_target_signal(ranges, velocities, phases, SCNR_dBs)\n",
    "            else:\n",
    "                # Just pass dummy dB array here; we won't scale inside 'generate_target_signal'\n",
    "                # Instead, we will do the scaling outside\n",
    "                S_raw = []\n",
    "                for i in range(n):\n",
    "                    # Each target can have the same 'gain' placeholder\n",
    "                    s_i = self.generate_target_signal(\n",
    "                        ranges[i].unsqueeze(-1),\n",
    "                        velocities[i].unsqueeze(-1),\n",
    "                        phases[i].unsqueeze(-1),\n",
    "                        gains_dB=torch.tensor([0.0])  # placeholder\n",
    "                    )\n",
    "                    S_raw.append(s_i)\n",
    "                # Sum all targets\n",
    "                S_unscaled = sum(S_raw)\n",
    "\n",
    "            # For each target, mark the closest range and Doppler bin.\n",
    "            for r, v in zip(ranges, velocities):\n",
    "                r_bin = torch.argmin(torch.abs(self.R - r))\n",
    "                v_bin = torch.argmin(torch.abs(self.V - v))\n",
    "                rd_label[r_bin, v_bin] = 1\n",
    "\n",
    "        # ---------------------------\n",
    "        # NEW: If snr & cnr are given, do amplitude scaling here\n",
    "        # ---------------------------\n",
    "        if (self.snr_dB is not None) and (self.cnr_dB is not None):\n",
    "            # 1) measure raw powers\n",
    "            noise_power  = W_unscaled.abs().pow(2).mean()\n",
    "            clutter_power= C_unscaled.abs().pow(2).mean() if C_unscaled.numel() > 0 else 0.0\n",
    "            signal_power = S_unscaled.abs().pow(2).mean() if S_unscaled.numel() > 0 else 0.0\n",
    "\n",
    "            # 2) define desired linear ratios\n",
    "            snr_lin = 10 ** (self.snr_dB / 10)\n",
    "            cnr_lin = 10 ** (self.cnr_dB / 10)\n",
    "\n",
    "            # 3) define desired final powers\n",
    "            #    We'll anchor the noise to \"1.0\" average power for convenience\n",
    "            #    (or you could anchor it to some other power). Then scale clutter & signal.\n",
    "            #    Step (A): Scale noise to final_noise_power = 1.0\n",
    "            #             => alpha_n = sqrt(1 / noise_power).\n",
    "            alpha_n = torch.sqrt(1.0 / noise_power)\n",
    "            W = alpha_n * W_unscaled  # final noise\n",
    "            final_noise_power = W.abs().pow(2).mean()\n",
    "\n",
    "            #    Step (B): Clutter should have average power = cnr_lin * final_noise_power\n",
    "            if clutter_power > 0:\n",
    "                alpha_c = torch.sqrt((cnr_lin * final_noise_power) / clutter_power)\n",
    "                C = alpha_c * C_unscaled\n",
    "            else:\n",
    "                C = torch.zeros_like(C_unscaled)\n",
    "\n",
    "            #    Step (C): Signal should have average power = snr_lin * final_noise_power\n",
    "            if signal_power > 0:\n",
    "                alpha_s = torch.sqrt((snr_lin * final_noise_power) / signal_power)\n",
    "                S = alpha_s * S_unscaled\n",
    "            else:\n",
    "                S = torch.zeros_like(S_unscaled)\n",
    "\n",
    "        else:\n",
    "            W = (W_unscaled / torch.sqrt(torch.tensor(self.sigma2)))  # old approach\n",
    "            C = C_unscaled\n",
    "            S = S_unscaled\n",
    "\n",
    "        X = S + C + W\n",
    "        \n",
    "        signal_energy  = S.abs().pow(2).sum()\n",
    "        clutter_energy = C.abs().pow(2).sum()\n",
    "        noise_energy   = W.abs().pow(2).sum()\n",
    "        scnr_lin = signal_energy / (clutter_energy + noise_energy + 1e-12)\n",
    "        scnr_dB  = 10.0 * torch.log10(scnr_lin + 1e-12)\n",
    "\n",
    "        return S, C, W, X, rd_label, scnr_dB\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Return the data and label for sample 'idx'.\n",
    "        \"\"\"\n",
    "        signal, clutter, gaus_noise, IQ, rd_label, scnr_dB = self.gen_frame_and_labels()\n",
    "        return signal, clutter, gaus_noise, IQ, rd_label, scnr_dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s, reproducible=False):\n",
    "    \"set random seed for torch, numpy and random\"\n",
    "    try: torch.manual_seed(s)\n",
    "    except NameError: pass\n",
    "    try: torch.cuda.manual_seed_all(s)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(s%(2**32-1)) #numpy demands seed between 0 and 2**32-1\n",
    "    except NameError: pass\n",
    "    random.seed(s)\n",
    "    if reproducible:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(args):\n",
    "    dataset_with_targets = RadarDataset(num_samples=args.num_samples, n_targets=args.n_targets, random_n_targets=True, snr=args.snr, cnr=args.cnr)\n",
    "    dataset_without = RadarDataset(num_samples=args.num_samples, n_targets=args.n_targets, random_n_targets=True, snr=args.snr, cnr=args.cnr)\n",
    "    full_dataset = ConcatDataset([dataset_with_targets, dataset_without])\n",
    "    train_size = int(0.9*len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folders(run_name):\n",
    "    \"create folders for models and results, and subfolders for each run\"\n",
    "    os.makedirs(\"Models\", exist_ok=True)\n",
    "    os.makedirs(\"Results\", exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"Models\", run_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"Results\", run_name), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_param(m):\n",
    "    \"get model first parameter\"\n",
    "    return next(iter(m.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"Exponential Moving Average, used to make training more stable\"\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    \n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "    \n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "    \n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True) # 4 attention heads, input tensors shape (batch, seq_len, emb_dim)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-1]\n",
    "        x = x.view(-1, self.channels, size * size).swapaxes(1, 2) # (batch, channels, H, W) -> (batch, channels, H*W) -> (batch, H*W, channels) which is required for MHD\n",
    "        x_ln = self.ln(x) # normalized input\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln) # compute self-attention where x_ln is query, key and value\n",
    "        attention_value = attention_value + x # residual connection\n",
    "        attention_value = self.ff_self(attention_value) + attention_value # feed forward network and residual connection again\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, size, size) # reshape back to (batch, channels, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(emb_dim, out_channels)\n",
    "        )\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:,:, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x+emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels//2)\n",
    "        )\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(emb_dim, out_channels)\n",
    "        )\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:,:,None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x+emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=2, c_out=2, time_dim=256, remove_deep_conv=False):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.remove_deep_conv = remove_deep_conv\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256)\n",
    "        if remove_deep_conv:\n",
    "            self.bot1 = DoubleConv(256, 256)\n",
    "            self.bot3 = DoubleConv(256, 256)\n",
    "        else:\n",
    "            self.bot1 = DoubleConv(256, 512)\n",
    "            self.bot2 = DoubleConv(512, 512)\n",
    "            self.bot3 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64)\n",
    "        self.up3 = Up(128,64)\n",
    "        self.sa6 = SelfAttention(64)\n",
    "        self.outc = DoubleConv(64, c_out)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=one_param(self).device).float()/channels))\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels //2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels //2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def unet_forward(self, x, t):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        if not self.remove_deep_conv:\n",
    "            x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        return self.unet_forward(x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_UNet(UNet):\n",
    "    def __init__(self, c_in=4, c_out=2, time_dim=256, **kwargs):\n",
    "        super().__init__(c_in, c_out, time_dim, **kwargs)\n",
    "        \n",
    "    def forward(self, x, cond_img, t):\n",
    "        t = t.unsqueeze(-1) if t.dim() == 1 else t\n",
    "        t = self.pos_encoding(t, self.time_dim)  # e.g., sinusoidal or learned pos enc\n",
    "        x_input = torch.cat([x, cond_img], dim=1)\n",
    "\n",
    "        return self.unet_forward(x_input, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    run_name = \"Conditional_Radar_Diffusion_v1\",\n",
    "    epochs = 100,\n",
    "    noise_steps =1000,\n",
    "    seed = 42,\n",
    "    batch_size = 10,\n",
    "    img_size = 64,\n",
    "    device = \"cuda\",\n",
    "    slice_size = 1,\n",
    "    do_validation = True,\n",
    "    fp16 = True,\n",
    "    log_every_epoch = 10,\n",
    "    num_workers=10,\n",
    "    lr = 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/arigranevich/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/Users/arigranevich/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/arigranevich/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/arigranevich/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'RadarDataset' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'RadarDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 18122, 18123) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 18123) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m val_mses \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_one_epoch(cond_diffusion, train_loader, optimizer, device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m     val_loss, gen_mse, gen_psnr \u001b[39m=\u001b[39m validate(cond_diffusion, val_loader, device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb Cell 22\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m diffusion\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m# Unpack the tuple from your dataset.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     signal, clutter, gaus_noise, IQ, rd_label, scnr_dB \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arigranevich/Developer/Research/Diffusion/notebooks/newdiftry.ipynb#X31sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mif\u001b[39;00m signal\u001b[39m.\u001b[39mreal\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Developer/Research/Diffusion/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{\u001b[39;00mpids_str\u001b[39m}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1260\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 18122, 18123) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "class ConditionalDiffusion(nn.Module):\n",
    "    def __init__(self, model, T=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        self.model = model  # instance of ConditionalUNet\n",
    "        self.T = T\n",
    "        self.register_buffer(\"betas\", torch.linspace(beta_start, beta_end, T))\n",
    "        self.register_buffer(\"alphas\", 1.0 - self.betas)\n",
    "        self.register_buffer(\"alpha_bars\", torch.cumprod(self.alphas, dim=0))\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: add noise to x0 at timestep t.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar = self.alpha_bars[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = (1 - self.alpha_bars[t]).sqrt().view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "    def p_losses(self, x0, t, cond):\n",
    "        \"\"\"\n",
    "        Loss: train network to predict the noise added.\n",
    "        x0: clean signal (B,2,H,W) with channels [real, imag]\n",
    "        cond: conditioning (observed noisy IQ) (B,2,H,W) with channels [real, imag]\n",
    "        \"\"\"\n",
    "        x_noisy, noise = self.q_sample(x0, t)\n",
    "        t_norm = t.float() / self.T\n",
    "        \n",
    "        # Concatenate along channel dimension: [x_noisy, cond] -> (B,4,H,W)\n",
    "        model_input = torch.cat([x_noisy, cond], dim=1)\n",
    "        noise_pred = self.model(model_input, t_norm)\n",
    "        return F.mse_loss(noise_pred, noise)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, cond):\n",
    "        \"\"\"\n",
    "        One reverse diffusion step (from x_t to x_{t-1}).\n",
    "        \"\"\"\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        alphas_t = self.alphas[t].view(-1, 1, 1, 1)\n",
    "        alpha_bars_t = self.alpha_bars[t].view(-1, 1, 1, 1)\n",
    "        t_norm = (torch.tensor([t], device=x.device).float() / self.T).repeat(x.shape[0])\n",
    "\n",
    "        model_input = torch.cat([x, cond], dim=1)\n",
    "        noise_pred = self.model(model_input, t_norm)\n",
    "\n",
    "        coef1 = 1 / torch.sqrt(alphas_t)\n",
    "        coef2 = betas_t / torch.sqrt(1 - alpha_bars_t)\n",
    "        mean = coef1 * (x - coef2 * noise_pred)\n",
    "        \n",
    "        noise = torch.randn_like(x) if t > 0 else 0\n",
    "        return mean + torch.sqrt(betas_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, shape):\n",
    "        \"\"\"\n",
    "        Generate a denoised signal conditioned on cond.\n",
    "        cond: (B,2,H,W) the observed noisy IQ (real and imaginary)\n",
    "        shape: desired shape of x (B,2,H,W)\n",
    "        \"\"\"\n",
    "        x = torch.randn(shape, device=cond.device)\n",
    "        for t in reversed(range(self.T)):\n",
    "            t_tensor = torch.tensor([t], device=x.device)\n",
    "            x = self.p_sample(x, t_tensor, cond)\n",
    "        return x\n",
    "    \n",
    "def train_one_epoch(diffusion, dataloader, optimizer, device):\n",
    "    diffusion.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Unpack the tuple from your dataset.\n",
    "        signal, clutter, gaus_noise, IQ, rd_label, scnr_dB = batch\n",
    "\n",
    "        if signal.real.ndim == 3:\n",
    "            x0_real = signal.real.unsqueeze(1)\n",
    "            x0_imag = signal.imag.unsqueeze(1)\n",
    "            cond_real = IQ.real.unsqueeze(1)\n",
    "            cond_imag = IQ.imag.unsqueeze(1)\n",
    "        else:\n",
    "            x0_real = signal.real\n",
    "            x0_imag = signal.imag\n",
    "            cond_real = IQ.real\n",
    "            cond_imag = IQ.imag\n",
    "\n",
    "        # Concatenate to form 2-channel tensors.\n",
    "        x0 = torch.cat([x0_real, x0_imag], dim=1).to(device)   # (B,2,H,W)\n",
    "        cond = torch.cat([cond_real, cond_imag], dim=1).to(device)  # (B,2,H,W)\n",
    "\n",
    "        # Sample random timesteps for diffusion.\n",
    "        t = torch.randint(0, diffusion.T, (x0.shape[0],), device=device).long()\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = diffusion.p_losses(x0, t, cond)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(diffusion, dataloader, device):\n",
    "    diffusion.eval()\n",
    "    val_loss = 0\n",
    "    gen_mse, gen_psnr = None, None\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        signal, clutter, gaus_noise, IQ, rd_label, scnr_dB = batch\n",
    "        \n",
    "        if signal.real.ndim == 3:\n",
    "            x0_real = signal.real.unsqueeze(1)\n",
    "            x0_imag = signal.imag.unsqueeze(1)\n",
    "            cond_real = IQ.real.unsqueeze(1)\n",
    "            cond_imag = IQ.imag.unsqueeze(1)\n",
    "        else:\n",
    "            x0_real = signal.real\n",
    "            x0_imag = signal.imag\n",
    "            cond_real = IQ.real\n",
    "            cond_imag = IQ.imag\n",
    "\n",
    "        x0 = torch.cat([x0_real, x0_imag], dim=1).to(device)  # (B,2,H,W)\n",
    "        cond = torch.cat([cond_real, cond_imag], dim=1).to(device)  # (B,2,H,W)\n",
    "        \n",
    "        t = torch.randint(0, diffusion.T, (x0.shape[0],), device=device).long()\n",
    "        loss = diffusion.p_losses(x0, t, cond)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # For the first batch, generate a sample and compute metrics.\n",
    "        if i == 0:\n",
    "            generated = diffusion.sample(cond, x0.shape)\n",
    "            mse_val = F.mse_loss(generated, x0).item()\n",
    "            psnr_val = 20 * math.log10(x0.max().item() / math.sqrt(mse_val)) if mse_val > 0 else 100\n",
    "            gen_mse, gen_psnr = mse_val, psnr_val\n",
    "\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    return avg_val_loss, gen_mse, gen_psnr\n",
    "\n",
    "full_dataset = RadarDataset(num_samples=102400, n_targets=8, random_n_targets=True, snr=30, cnr=15)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "train_dataset_with_targets = RadarDataset(num_samples=102400, n_targets=8, random_n_targets=True, snr=30, cnr=15)\n",
    "train_dataset_no_targets = RadarDataset(num_samples=10240, n_targets=0, random_n_targets=False, snr=30, cnr=15)\n",
    "train_dataset = ConcatDataset([train_dataset_with_targets, train_dataset_no_targets])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2,\n",
    "                          pin_memory=torch.cuda.is_available(), persistent_workers=True)\n",
    "\n",
    "cond_unet = Conditional_UNet().to(device)\n",
    "cond_diffusion = ConditionalDiffusion(model=cond_unet, T=1000, beta_start=1e-4, beta_end=0.02).to(device)\n",
    "# unet_model = Unet(dim=64, channels=4, out_dim=2, with_time_emb=True).to(device)\n",
    "\n",
    "# cond_diffusion = ConditionalDiffusion(\n",
    "#     model=unet_model,\n",
    "#     T=1000,\n",
    "#     beta_start=1e-4,\n",
    "#     beta_end=0.02\n",
    "# ).to(device)\n",
    "optimizer = torch.optim.Adam(cond_diffusion.parameters(), lr=1e-5)\n",
    "num_epochs = 220\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_psnrs = []\n",
    "val_mses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(cond_diffusion, train_loader, optimizer, device)\n",
    "    val_loss, gen_mse, gen_psnr = validate(cond_diffusion, val_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if gen_psnr is not None:\n",
    "        val_psnrs.append(gen_psnr)\n",
    "    if gen_mse is not None:\n",
    "        val_mses.append(gen_mse)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}\")\n",
    "    if gen_mse is not None and gen_psnr is not None:\n",
    "        print(f\"   [Generation Metrics] MSE: {gen_mse:.4f} | PSNR: {gen_psnr:.2f} dB\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(cond_diffusion.state_dict(), \"new_diffusion.pth\")\n",
    "        print(\"   --> Best model saved.\")\n",
    "\n",
    "# 5. Plot training curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "if len(val_psnrs) > 0:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(val_psnrs, label=\"Val PSNR (dB)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"PSNR\")\n",
    "    plt.title(\"Validation PSNR over epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, c_in=2, c_out=2, device=\"cuda\", **kwargs):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "        self.model = Conditional_UNet(c_in, c_out)\n",
    "        self.ema_model = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "        self.device = device\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "    \n",
    "    def noise_images(self, x, t):\n",
    "        \"add noise to images at instant t\"\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:,None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1-self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.rand_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "    \n",
    "    def train_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.ema.step_ema(self.ema_model, self.model)\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def load(self, model_ckpt_path, model_ckpt=\"ckpt.pt\", ema_model_ckpt=\"ema_ckpt.pt\"):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(model_ckpt_path, model_ckpt)))\n",
    "        self.ema_model.load_state_dict(torch.load(os.path.join(model_ckpt_path, ema_model_ckpt)))\n",
    "\n",
    "    def save_model(self, run_name, epoch=-1):\n",
    "        \"save model locally\"\n",
    "        torch.save(self.model.state_dict(), os.path.join(\"models\", run_name, f\"ckpt.pt\"))\n",
    "        torch.save(self.ema_model.state_dict(), os.path.join(\"models\", run_name, f\"ema_ckpt.pt\"))\n",
    "        torch.save(self.optimizer.state_dict(), os.path.join(\"models\", run_name, f\"optim.pt\"))\n",
    "\n",
    "    def prepare(self, args):\n",
    "        make_folders(args.run_name)\n",
    "        self.train_dataloader, self.val_dataloader = get_data(args)\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=args.lr, eps=1e-5)\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.lr, steps_per_epoch=len(self.train_dataloader), epochs=args.epochs)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ema = EMA(0.995)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def fit(self, args):\n",
    "        for epoch in progress_bar(range(args.epochs), total=args.epochs, leave=True):\n",
    "            _ = self.one_epoch(train=True)\n",
    "\n",
    "            if args.do_validation:\n",
    "                avg_loss = self.one_epoch(train=False)\n",
    "            \n",
    "        self.save_model(run_name=args.run_name, epoch=epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def one_epoch(self, train=True):\n",
    "        avg_loss = 0.\n",
    "        if train: self.model.train()\n",
    "        else: self.model.eval()\n",
    "        pbar = progress_bar(self.train_dataloader, leave=False)\n",
    "        for i, (images, nimages) in enumerate(pbar):\n",
    "            with torch.autocast(\"cuda\") and (torch.inference_mode() if not train else torch.enable_grad()):\n",
    "                images = images.to(self.device)\n",
    "                nimages = nimages.to(self.device)\n",
    "                t = self.sample_timesteps(images.shape[0]).to(self.device)\n",
    "                x_t, noise = self.noise_images(images, t)\n",
    "                predicted_noise = self.model(x_t, t, nimages)\n",
    "                loss = self.mse(noise, predicted_noise)\n",
    "                avg_loss += loss\n",
    "            if train:\n",
    "                self.train_step(loss)\n",
    "            pbar.comment = f\"MSE={loss.item():2.3f}\"\n",
    "        return avg_loss.mean().item()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
